# -*- coding: utf-8 -*-
"""DP-GAN (Differentially Private Generative Adversarial Network).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WXd0t7P65SQkRCsNUPsOewYYVTDrnFbq

# Sintesis Data dengan Library synthcity
Lakukan sintesis data dengan library synthcity dengan plugin Bayesian Network, sintesis berjumlah 500 sampel

## Data loading

### Subtask:
Load the dataset "data_terbaru.csv" into a pandas DataFrame.

**Reasoning**:
Load the dataset "data_terbaru.csv" into a pandas DataFrame and display its first few rows and shape.
"""

import pandas as pd

try:
    df = pd.read_csv('data_terbaru.csv')
    display(df.head())
    print(df.shape)
except FileNotFoundError:
    print("Error: 'data_terbaru.csv' not found.")
    df = None
except pd.errors.ParserError:
    print("Error: Could not parse 'data_terbaru.csv'. Check file format.")
    df = None
except Exception as e:
    print(f"An unexpected error occurred: {e}")
    df = None



"""## Data preparation

### Subtask:
Prepare the loaded data for synthetic data generation using the synthcity library.

**Reasoning**:
Handle missing values, encode categorical features, validate data types, and create a copy of the modified DataFrame.
"""

# Check for missing values
print(df.isnull().sum())

# No missing values found in the provided head, so no imputation or removal is needed.

# Identify categorical features and apply one-hot encoding if necessary.
# Based on the provided head, 'GRUP' might be categorical.  Check unique values.
print(df['GRUP'].unique())

# If 'GRUP' has a limited number of unique values, it's categorical.  Let's assume it is.
# We'll use one-hot encoding.  If 'GRUP' contains more unique values, revise accordingly.

# One-hot encode 'GRUP'
df = pd.get_dummies(df, columns=['GRUP'], prefix=['GRUP'])

# Data Type Validation (ensure numerical columns are numeric)
for col in ['BPMJ', 'BPMP', 'SPOJ', 'SPOP', 'SUHUJ', 'SUHUP']:
    if not pd.api.types.is_numeric_dtype(df[col]):
        try:
            df[col] = pd.to_numeric(df[col], errors='coerce')
            print(f"Column '{col}' converted to numeric.")
        except Exception as e:
            print(f"Error converting '{col}' to numeric: {e}")


# Create a copy of the modified DataFrame
df_prepared = df.copy()

display(df_prepared.head())

"""## Model training

### Subtask:
Train a Bayesian Network model using the synthcity library on the prepared data.
"""

import pandas as pd
real = pd.read_csv("/content/data_terbaru.csv")

from synthcity.plugins import Plugins
Plugins(categories=["generic", "privacy"]).list()

syn_model_dpgan = Plugins().get("dpgan")
syn_model_dpgan .fit(real)



"""## Model  Evaluation

### KS Test
"""

# Periksa statistik deskriptif dari data asli
print("Statistik Data Asli:")
print(real.describe())

# Periksa statistik deskriptif dari data sintetis
print("\nStatistik Data Sintetis:")
synthetic_dpgan = synthetic_dpgan.dataframe()
print(synthetic_dpgan.describe())

import pandas as pd
from scipy.stats import ks_2samp

# Membuat DataFrame
dt_real = pd.DataFrame(real)
dt_synthetic = pd.DataFrame(synthetic_dpgan)

# Melakukan uji Kolmogorov-Smirnov untuk setiap kolom
ks_results_dp = {col: ks_2samp(dt_real[col], dt_synthetic[col]) for col in real.keys()}

ks_results_dp

from synthcity.utils.serialization import load, load_from_file, save, save_to_file
buff = save(syn_model_dpgan)
type(buff)

reloaded = load(buff)
reloaded.name()

synthetic_dpgan.to_csv('sampel000_dpgan.csv', index=True)

from synthcity.plugins.core.dataloader import DataLoader

from synthcity.plugins.core.models.tabular_gan import TabularGAN

from synthcity.plugins.core.dataloader import GenericDataLoader

generic_loader = GenericDataLoader(real)

torch_dataloader = load(buff)

from synthcity.plugins.core.dataloader import GenericDataLoader
from synthcity.benchmark import Benchmarks
import pprint

# Bungkus data asli dan sintetis
real_loaderrrr = GenericDataLoader(dt_real, target_column=None)
synthetic_loaderrrr = GenericDataLoader(synthetic_dpgan, target_column=None)

# Tes dan metrik
tests = [("custom_generator", "synthetic", synthetic_loaderrrr)]  # custom_generator = nama plugin bebas
metrics = {
    "statistical": {},
    "privacy": {}
}

from synthcity.benchmark import Benchmarks

results_dp = Benchmarks.evaluate(
    tests=[("marginal_distributions", "marginal_distributions", {})],
    X=real_loaderrrr,  # ini data asli
    synthetic=synthetic_loaderrrr,
    synthetic_size=1000,
    repeats=1
)


# Tampilkan hasil
pprint.pprint(results_dp)

"""### Corelattion Matrix

Mengukur hubungan linier antar variabel numerik. Kita bandingkan korelasi antar kolom di data asli vs sintetis. Jika beda jauh ‚Üí struktur hubungan rusak.

* Di dalam data sintetis itu sendiri (apakah masuk akal dan konsisten), dan

* Antara data sintetis dan data asli (apakah pola-pola hubungan antarkolom tetap terjaga).
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load data
df_real = pd.read_csv("data_terbaru.csv")
df_synth = pd.read_csv("/content/sampel000_dpgan.csv")

# Hitung korelasi
corr_real = df_real.corr(numeric_only=True)
corr_synth = df_synth.corr(numeric_only=True)

df_synth.shape

# Plot perbandingan korelasi
fig, axs = plt.subplots(1, 2, figsize=(12, 5))
sns.heatmap(corr_real, ax=axs[0], annot=True, cmap="Blues", cbar=False)
axs[0].set_title("Korelasi Data Asli")

sns.heatmap(corr_synth, ax=axs[1], annot=True, cmap="Greens", cbar=False)
axs[1].set_title("Korelasi Data Sintetis (dp gan)")

plt.tight_layout()
plt.show()

"""### PCA (Principal Component Analysis)

Mengurangi dimensi & memvisualisasikan data. Jika pola distribusi komponen utama berbeda antara asli dan sintetis ‚Üí struktur data berbeda.
"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Reload data
df_real = pd.read_csv("data_terbaru.csv")
df_synth = pd.read_csv("/content/sampel000_dpgan.csv")

# Ambil hanya kolom numerik dan drop NaN jika ada
# Select only the common numeric columns for scaling
df_real_num = df_real.select_dtypes(include=['number']).dropna()
df_synth_num = df_synth.select_dtypes(include=['number']).dropna()
# Standarisasi data sebelum PCA
scaler = StandardScaler()
real_scaled = scaler.fit_transform(df_real_num)
synth_scaled = scaler.transform(df_synth_num)

# PCA - Ambil 2 komponen utama
pca = PCA(n_components=2)
real_pca = pca.fit_transform(real_scaled)
synth_pca = pca.transform(synth_scaled)

# Plotting hasil PCA
plt.figure(figsize=(10, 6))
plt.scatter(real_pca[:, 0], real_pca[:, 1], alpha=0.6, label='Data Asli', color='blue')
plt.scatter(synth_pca[:, 0], synth_pca[:, 1], alpha=0.6, label='Data Sintetis (dp gan)', color='green')
plt.xlabel('Komponen Utama 1')
plt.ylabel('Komponen Utama 2')
plt.title('Visualisasi PCA (2D) Data Asli vs Sintetis')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""### TSTR (Train on Synthetic, Test on Real)

Hasil data augmentasi dijadikan data training sedangkan data asli jadi data testing.

üß† Tujuan Uji Ini: Kita ingin tahu apakah data sintetik (hasil augmentasi) cukup berkualitas untuk melatih model yang tetap akurat saat diuji di data asli.

Dengan begitu, kamu bisa percaya bahwa data hasil augmentasi ini benar-benar punya "nilai fungsional".

#### RF
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report

df_real = pd.read_csv("/content/data_terbaru.csv")
df_dpgan = pd.read_csv("/content/sampel000_dpgan.csv")

def get_X_y(df):
    X = df.drop(columns=["GRUP"])
    y = df["GRUP"]
    return X, y

X_real, y_real = get_X_y(df_real)
X_df_dpgan, y_df_dpgan = get_X_y(df_dpgan)

# Get the common columns between the two DataFrames
common_cols = X_real.columns.intersection(X_df_dpgan.columns)

# Select only these common columns for scaling
X_df_dpgan_scaled = scaler.fit_transform(X_df_dpgan[common_cols])
X_real_scaled = scaler.transform(X_real[common_cols])

# ü§ñ Model: Train di Bayesian, Test di Data Asli
# ======================
model_dpgan = RandomForestClassifier(random_state=42)
model_dpgan.fit(X_df_dpgan_scaled, y_df_dpgan) # Changed y_bn to y_df_bn

# Drop 'Unnamed: 0' before fitting
#X_bn = X_bn.drop(columns=['Unnamed: 0'], errors='ignore')
# errors='ignore' prevents error if the column doesn't exist

# üß™ Test di data asli
y_pred_dpgan = model_dpgan.predict(X_real_scaled)

# üìä Evaluasi performa
print("üìä Performance (Train on dpgan ‚Üí Test on REAL):")
print(classification_report(y_real, y_pred_dpgan))

"""**HYPERPARAMETER TUNING**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

# Inisialisasi model dasar
model_rf = RandomForestClassifier(random_state=42)

# Ruang hyperparameter
grid_space = {
    'max_depth': [3, 5, 10, None],
    'n_estimators': [10, 100, 200],
    'max_features': [1, 3, 5, 7],
    'min_samples_leaf': [1, 2, 3],
    'min_samples_split': [2, 3]  # ‚Üê fix: tidak boleh 1
}

# Grid Search dengan cross-validation 3-fold
grid = GridSearchCV(
    model_rf,
    param_grid=grid_space,
    cv=3,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)
# Lakukan tuning: train di data sintetik BAYESIAN
grid.fit(X_df_dpgan_scaled, y_df_dpgan)

# Gunakan model terbaik untuk prediksi di data asli
best_model = grid.best_estimator_
y_pred = best_model.predict(X_real_scaled)

# Tampilkan hasil evaluasi
print("üìà Best Params:", grid.best_params_)
print("\nüìä Evaluation (After Tuning):")
print(classification_report(y_real, y_pred))

"""#### LR"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

df_real = pd.read_csv("/content/data_terbaru.csv")
df_dpgan= pd.read_csv("/content/sampel000_dpgan.csv")

# Fungsi bantu tetap sama
def get_X_y(df):
    X = df.drop(columns=["GRUP"])
    y = df["GRUP"]
    return X, y

# Ambil data training & testing
X_real, y_real = get_X_y(df_real)
X_dpgan, y_dpgan = get_X_y(df_dpgan)

# Bersihkan kolom jika ada 'Unnamed: 0'
X_dpgan = X_dpgan.drop(columns=['Unnamed: 0'], errors='ignore')
X_real = X_real.drop(columns=['Unnamed: 0'], errors='ignore')

# üîÑ Standardisasi fitur
scaler = StandardScaler()
X_dpgan_scaled = scaler.fit_transform(X_dpgan)
X_real_scaled = scaler.transform(X_real)

# üîß Inisialisasi model LogisticRegression
model_logreg = LogisticRegression(max_iter=1000, solver='liblinear')

# ü§ñ Train di data sintetik
model_logreg.fit(X_dpgan_scaled, y_dpgan)

# üß™ Uji di data asli
y_pred_logreg = model_logreg.predict(X_real_scaled)

# Tampilkan hasil evaluasi
print("üìä Performance (Train on dpgan ‚Üí Test on REAL) - LogisticRegression:")
print(classification_report(y_real, y_pred_logreg))

"""**HYPERPARAMETER TUNING**"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

# Bersihkan jika ada kolom tambahan
X_dpgan = X_dpgan.drop(columns=['Unnamed: 0'], errors='ignore')
X_real = X_real.drop(columns=['Unnamed: 0'], errors='ignore')

# Definisikan model dasar
logreg = LogisticRegression()

# Parameter grid untuk tuning
param_grid = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear'],  # liblinear support l1 & l2
    'max_iter': [100, 300, 1000]
}

# Inisialisasi GridSearch
grid = GridSearchCV(
    estimator=logreg,
    param_grid=param_grid,
    scoring='accuracy',
    cv=3,
    verbose=1,
    n_jobs=-1
)
# üöÄ Tuning dengan data sintetik

grid.fit(X_df_dpgan_scaled, y_df_dpgan)

# Prediksi di data asli
best_logreg = grid.best_estimator_
y_pred_tuned = best_logreg.predict(X_real)

# Hasil akhir
print("üìå Best Params:", grid.best_params_)
print("\nüìä Performance (Logistic Regression TUNED ‚Äî Train on dpgan ‚Üí Test on REAL):")
print(classification_report(y_real, y_pred_tuned))

"""#### SVM"""

from sklearn.svm import SVC
from sklearn.metrics import classification_report

# Fungsi bantu
def get_X_y(df):
    X = df.drop(columns=["GRUP"])
    y = df["GRUP"]
    return X, y

# Ambil data training & testing
X_real, y_real = get_X_y(df_real)
X_dpgan, y_dpgan = get_X_y(df_dpgan)

# Bersihkan kolom 'Unnamed: 0' jika ada
X_dpgan = X_dpgan.drop(columns=['Unnamed: 0'], errors='ignore')
X_real = X_real.drop(columns=['Unnamed: 0'], errors='ignore')

# Inisialisasi model SVM
model_svm = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)

# üöÄ Train model di data sintetik (DP GAN)
model_svm.fit(X_dpgan, y_dpgan)

# üß™ Test model di data asli
y_pred_svm = model_svm.predict(X_real)

# üìä Tampilkan hasil evaluasi
print("üìä Performance (SVM ‚Äî Train on dpgan ‚Üí Test on REAL):")
print(classification_report(y_real, y_pred_svm))

"""**HYPERPARAMETER TUNING**"""

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

# Bersihkan kolom jika ada 'Unnamed: 0'
X_dpgan = X_dpgan.drop(columns=['Unnamed: 0'], errors='ignore')
X_real = X_real.drop(columns=['Unnamed: 0'], errors='ignore')

# Definisikan model dasar
svm = SVC()

# Parameter grid
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.01, 0.1, 1],
    'kernel': ['rbf', 'linear']
}

# Setup GridSearch
grid = GridSearchCV(
    estimator=svm,
    param_grid=param_grid,
    scoring='accuracy',
    cv=3,
    verbose=1,
    n_jobs=-1
)

# üöÄ Training model di data sintetik (Bayesian)
grid.fit(X_df_dpgan_scaled, y_df_dpgan)

# üß™ Uji di data asli
best_svm = grid.best_estimator_
y_pred_tuned_svm = best_svm.predict(X_real)

# üìù Hasil
print("üìå Best Hyperparameters (SVM):", grid.best_params_)
print("\nüìä Performance (TUNED SVM ‚Äî Train on dpgan ‚Üí Test on REAL):")
print(classification_report(y_real, y_pred_tuned_svm))



"""## Part 2

### RF
"""

# Pisahkan fitur dan target
X_train = df_dpgan.drop(columns=['GRUP'])   # asumsi kolom target bernama 'target'
y_train = df_dpgan['GRUP']

X_test = df_real.drop(columns=['GRUP'])
y_test = df_real['GRUP']

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# Normalisasi fitur
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.model_selection import GridSearchCV
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [5, 10, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
}

rf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                           cv=3, scoring='accuracy', n_jobs=-1)

grid_search.fit(X_train_scaled, y_train)

# Evaluasi terbaik
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test_scaled)
print("Best Parameters:", grid_search.best_params_)
print("Akurasi:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
import seaborn as sns

# Pastikan X_train dan X_test sudah di-scaling
# Kalau belum, pakai df_dpgan.drop(columns=['GRUP']) dan df_real.drop(columns=['GRUP'])

for col in X_train.columns:
    plt.figure(figsize=(6, 3))
    sns.kdeplot(X_train[col], label='DPGAN (Train)', fill=True)
    sns.kdeplot(X_test[col], label='REAL (Test)', fill=True)
    plt.title(f'Distribusi Fitur: {col}')
    plt.legend()
    plt.tight_layout()
    plt.show()

from sklearn.model_selection import train_test_split

# Split real data menjadi validasi dan final test
X_val_raw, X_test_final_raw, y_val, y_test_final = train_test_split(
    df_real.drop(columns=['GRUP']), df_real['GRUP'],
    test_size=0.8, random_state=42
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(df_dpgan.drop(columns=['GRUP']))
X_val_scaled = scaler.transform(X_val_raw)
X_test_final_scaled = scaler.transform(X_test_final_raw)

y_train = df_dpgan['GRUP']

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [5, 10, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
}

best_score = 0
best_model = None
best_params = None

for n in param_grid['n_estimators']:
    for d in param_grid['max_depth']:
        for mss in param_grid['min_samples_split']:
            for msl in param_grid['min_samples_leaf']:
                model = RandomForestClassifier(
                    n_estimators=n,
                    max_depth=d,
                    min_samples_split=mss,
                    min_samples_leaf=msl,
                    random_state=42
                )
                model.fit(X_train_scaled, y_train)
                y_val_pred = model.predict(X_val_scaled)
                acc = accuracy_score(y_val, y_val_pred)
                if acc > best_score:
                    best_score = acc
                    best_model = model
                    best_params = {
                        'n_estimators': n,
                        'max_depth': d,
                        'min_samples_split': mss,
                        'min_samples_leaf': msl
                    }

print("Best Parameters (Validasi di data asli):", best_params)

y_test_pred = best_model.predict(X_test_final_scaled)

from sklearn.metrics import classification_report, confusion_matrix
print("Akurasi Final Test:", accuracy_score(y_test_final, y_test_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test_final, y_test_pred))
print("Classification Report:\n", classification_report(y_test_final, y_test_pred))



"""### LR"""

# Pisahkan fitur dan target
X_train = df_dpgan.drop(columns=['GRUP'])   # asumsi kolom target bernama 'target'
y_train = df_dpgan['GRUP']

X_test = df_real.drop(columns=['GRUP'])
y_test = df_real['GRUP']

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# Normalisasi fitur
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.pipeline import Pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('lr', LogisticRegression(max_iter=1000, random_state=42))
])

# Hyperparameter grid for Logistic Regression
param_grid_lr = {
    'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'lr__C': [0.01, 0.1, 1, 10],
    'lr__solver': ['saga'],  # 'saga' supports all types of penalties
    'lr__l1_ratio': [0, 0.5, 1]  # Only used if penalty is 'elasticnet'
}

# GridSearchCV setup
grid_search = GridSearchCV(pipeline, param_grid=param_grid_lr,
                           cv=5, scoring='accuracy', n_jobs=-1)

# Fit the grid search to your training data
grid_search.fit(X_train, y_train)

# Best params and score
print("Best Parameters:", grid_search.best_params_)
print("Best Accuracy Score:", grid_search.best_score_)

import matplotlib.pyplot as plt
import seaborn as sns

# Pastikan X_train dan X_test sudah di-scaling
# Kalau belum, pakai df_dpgan.drop(columns=['GRUP']) dan df_real.drop(columns=['GRUP'])

for col in X_train.columns:
    plt.figure(figsize=(6, 3))
    sns.kdeplot(X_train[col], label='DPGAN (Train)', fill=True)
    sns.kdeplot(X_test[col], label='REAL (Test)', fill=True)
    plt.title(f'Distribusi Fitur: {col}')
    plt.legend()
    plt.tight_layout()
    plt.show()

from sklearn.model_selection import train_test_split

# Split real data menjadi validasi dan final test
X_val_raw, X_test_final_raw, y_val, y_test_final = train_test_split(
    df_real.drop(columns=['GRUP']), df_real['GRUP'],
    test_size=0.8, random_state=42
)

# Scaling (wajib untuk regresi logistik agar hasil lebih stabil)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val_raw)

# Hyperparameter grid untuk Logistic Regression
penalties = ['l1', 'l2', 'elasticnet', 'none']
Cs = [0.01, 0.1, 1, 10]
solvers = ['saga']  # Gunakan 'saga' karena mendukung semua jenis penalti
l1_ratios = [0.0, 0.5, 1.0]  # Hanya digunakan jika penalty='elasticnet'

best_score = 0
best_model = None
best_params = None

for penalty in penalties:
    for C in Cs:
        for solver in solvers:
            # Untuk 'elasticnet', kita coba l1_ratio
            if penalty == 'elasticnet':
                for l1_ratio in l1_ratios:
                    try:
                        model = LogisticRegression(
                            penalty=penalty,
                            C=C,
                            solver=solver,
                            l1_ratio=l1_ratio,
                            max_iter=1000,
                            random_state=42
                        )
                        model.fit(X_train_scaled, y_train)
                        y_val_pred = model.predict(X_val_scaled)
                        acc = accuracy_score(y_val, y_val_pred)
                        if acc > best_score:
                            best_score = acc
                            best_model = model
                            best_params = {
                                'penalty': penalty,
                                'C': C,
                                'solver': solver,
                                'l1_ratio': l1_ratio
                            }
                    except Exception as e:
                        continue  # Lewati kombinasi yang tidak valid
            else:
                try:
                    model = LogisticRegression(
                        penalty=penalty,
                        C=C,
                        solver=solver,
                        max_iter=1000,
                        random_state=42
                    )
                    model.fit(X_train_scaled, y_train)
                    y_val_pred = model.predict(X_val_scaled)
                    acc = accuracy_score(y_val, y_val_pred)
                    if acc > best_score:
                        best_score = acc
                        best_model = model
                        best_params = {
                            'penalty': penalty,
                            'C': C,
                            'solver': solver
                        }
                except Exception as e:
                    continue  # Lewati kombinasi yang tidak valid

print("Best Parameters (Validasi di data asli):", best_params)
print("Best Accuracy:", best_score)

from sklearn.metrics import classification_report

# Prediksi di validation set
y_val_pred = best_model.predict(X_val_scaled)

# Classification report
print("\nClassification Report (Validation Set):")
print(classification_report(y_val, y_val_pred))



"""### SVM"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler

# 1. Pisahkan fitur dan target
X_train = df_dpgan.drop(columns=['GRUP'])  # asumsi kolom target = 'GRUP'
y_train = df_dpgan['GRUP']

X_test = df_real.drop(columns=['GRUP'])
y_test = df_real['GRUP']

# 2. Normalisasi fitur
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 3. Hyperparameter grid untuk SVM
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto']
}

# 4. Setup GridSearchCV dengan SVC
svc = SVC(probability=True, random_state=42)
grid_search = GridSearchCV(estimator=svc, param_grid=param_grid,
                           cv=3, scoring='accuracy', n_jobs=-1)

# 5. Fit model ke data training
grid_search.fit(X_train_scaled, y_train)

# 6. Evaluasi model terbaik
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test_scaled)

print("Best Parameters:", grid_search.best_params_)
print("Akurasi:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
import seaborn as sns

# Pastikan X_train dan X_test sudah di-scaling
# Kalau belum, pakai df_dpgan.drop(columns=['GRUP']) dan df_real.drop(columns=['GRUP'])

for col in X_train.columns:
    plt.figure(figsize=(6, 3))
    sns.kdeplot(X_train[col], label='DPGAN (Train)', fill=True)
    sns.kdeplot(X_test[col], label='REAL (Test)', fill=True)
    plt.title(f'Distribusi Fitur: {col}')
    plt.legend()
    plt.tight_layout()
    plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# 1. Split data real menjadi validasi dan final test
X_val_raw, X_test_final_raw, y_val, y_test_final = train_test_split(
    df_real.drop(columns=['GRUP']), df_real['GRUP'],
    test_size=0.8, random_state=42
)

# 2. Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(df_dpgan.drop(columns=['GRUP']))
X_val_scaled = scaler.transform(X_val_raw)
X_test_final_scaled = scaler.transform(X_test_final_raw)
y_train = df_dpgan['GRUP']

# 3. Hyperparameter grid untuk SVC
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto']
}

best_score = 0
best_model = None
best_params = None

# 4. Manual grid search
for C in param_grid['C']:
    for kernel in param_grid['kernel']:
        for gamma in param_grid['gamma']:
            try:
                model = SVC(C=C, kernel=kernel, gamma=gamma, probability=True, random_state=42)
                model.fit(X_train_scaled, y_train)
                y_val_pred = model.predict(X_val_scaled)
                acc = accuracy_score(y_val, y_val_pred)
                if acc > best_score:
                    best_score = acc
                    best_model = model
                    best_params = {
                        'C': C,
                        'kernel': kernel,
                        'gamma': gamma
                    }
            except Exception as e:
                continue

# 5. Hasil Validasi
print("Best Parameters (Validasi di data asli):", best_params)

# 6. Evaluasi di final test
y_test_pred = best_model.predict(X_test_final_scaled)
print("Akurasi Final Test:", accuracy_score(y_test_final, y_test_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test_final, y_test_pred))
print("Classification Report:\n", classification_report(y_test_final, y_test_pred))